{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"glove_attn_task3.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1PHba8-NsQABY59EAValxr4Z0MaieofoG","authorship_tag":"ABX9TyOus27jIorY/3wfxF6v+7EH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BMOjUkSLf3Ux","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593346170416,"user_tz":-330,"elapsed":1674,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"bf53c36c-851b-4f0d-9818-aafadb52c879"},"source":["%tensorflow_version 1.x #use tensorflow magic to use version 1.x in colab"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Usom7M_bFh5a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593346176628,"user_tz":-330,"elapsed":7831,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"3fdbc5ad-27ff-4a76-9153-60b89824a521"},"source":["#imports\n","import pandas as pd\n","from keras.preprocessing import text as keras_text, sequence as keras_seq\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n","from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n","from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n","from keras.optimizers import Adam\n","from keras.models import Model\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.utils import np_utils\n","\n","from keras.layers import *\n","from keras.models import *\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.initializers import *\n","from keras.optimizers import *\n","import keras.backend as K\n","from keras.callbacks import *\n","import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"I9KtFhjfFk-s","colab_type":"code","colab":{}},"source":["train=pd.read_csv(\"/content/drive/My Drive/minor/english_dataset/eng23train.csv\") #traning data\n","test=pd.read_csv(\"/content/drive/My Drive/minor/english_dataset/eng23test.csv\")  #testing data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4th6FGtFyHJ","colab_type":"code","colab":{}},"source":["train_X=train['text']  #training text\n","test_X=test['text']   #testing text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGbg5f_AIyQR","colab_type":"code","colab":{}},"source":["embed_size = 200 # how big is each word vector\n","maxlen = 70 # max number of words in a question to use"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RJyyZMh_eE2","colab_type":"code","colab":{}},"source":["\n","## Tokenize the sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(list(train_X)+list(test_X))\n","train_X = tokenizer.texts_to_sequences(train_X)\n","test_X = tokenizer.texts_to_sequences(test_X)\n","word_index = tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwT1bHCiT1z3","colab_type":"code","colab":{}},"source":["max_features = len(word_index) # how many unique words to use (i.e num rows in embedding vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY_HHF4fGGZR","colab_type":"code","colab":{}},"source":["train_X =pad_sequences(train_X, maxlen=maxlen)  #padding training text to length=70\n","test_X = pad_sequences(test_X, maxlen=maxlen)  #padding testing text to length=70\n","train_Y=train['task_3']    #training label\n","test_Y=test['task_3']       #testing label\n","train_Y=np_utils.to_categorical(train_Y)   #one-hot encoded training label\n","test_Y=np_utils.to_categorical(test_Y)   #one-hot encoded testing label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_E8MwYawH9Ih","colab_type":"code","colab":{}},"source":["def load_glove(word_index):\n","    EMBEDDING_FILE = '/content/drive/My Drive/minor/glove/glove.twitter.27B.200d.txt'   #glove file\n","    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n","    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))     #open glove embediing file\n","    \n","    all_embs = np.stack(embeddings_index.values())\n","    emb_mean,emb_std = -0.005838499,0.48782197   #embedding mean, standard deviation for intializing not found vectors\n","    embed_size = all_embs.shape[1]\n","\n","    # word_index = tokenizer.word_index\n","    nb_words = min(max_features, len(word_index))\n","    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n","    for word, i in word_index.items():\n","        if i >= max_features: continue\n","        embedding_vector = embeddings_index.get(word) #get vector for ith word\n","        if embedding_vector is not None: \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            embedding_vector = embeddings_index.get(word.capitalize())  #if vector for lowercase notfound then try for uppercase\n","            if embedding_vector is not None: \n","                embedding_matrix[i] = embedding_vector\n","    return embedding_matrix "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyGkhEJrQyGL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593346261453,"user_tz":-330,"elapsed":92507,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"2f6d7599-b12c-4acf-9c11-cee4477893ba"},"source":["embedding_matrix = load_glove(word_index)  #create embedding matrix using glove"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FN6_TKHst72Y","colab_type":"code","colab":{}},"source":["#imlementation of attention layer in keras in this cell\n","def dot_product(x, kernel):\n","    \"\"\"\n","    Wrapper for dot product operation, in order to be compatible with both\n","    Theano and Tensorflow\n","    Args:\n","        x (): input\n","        kernel (): weights\n","    Returns:\n","    \"\"\"\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","    \n","\n","class AttentionWithContext(Layer):\n","    \"\"\"\n","    Attention operation, with a context/query vector, for temporal data.\n","    Supports Masking.\n","    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n","    \"Hierarchical Attention Networks for Document Classification\"\n","    by using a context vector to assist the attention\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    How to use:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Note: The layer has been tested with Keras 2.0.6\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(AttentionWithContext())\n","        # next add a Dense layer (for classification/regression) or whatever...\n","    \"\"\"\n","\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight(shape=(input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight(shape=(input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        # apply mask after the exp. will be re-normalized next\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        # in some cases especially in the early stages of training the sum may be almost zero\n","        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n","        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSrJna2Ych4C","colab_type":"code","colab":{}},"source":["def model_lstm_du(embedding_matrix):\n","    inp = Input(shape=(maxlen,)) #input layer\n","    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n","    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x) #bidirectional lstm layer optimized for gpu\n","    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n","    x = AttentionWithContext()(x)  #attention layer\n","    x = Dense(64, activation=\"relu\")(x)\n","    outp = Dense(2, activation=\"sigmoid\")(x) #output layer\n","    model = Model(inputs=inp, outputs=outp)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2H5m8yQem6N","colab_type":"code","colab":{}},"source":["def model_gru_du(embedding_matrix):\n","    inp = Input(shape=(maxlen,)) #input layer\n","    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n","    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x) #bidirectional gru layer optimized for gpu\n","    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n","    x = AttentionWithContext()(x)  #attention layer\n","    x = Dense(64, activation=\"relu\")(x)\n","    outp = Dense(2, activation=\"sigmoid\")(x) #output layer\n","    model = Model(inputs=inp, outputs=outp)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqQPL7Fwcmq-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"ok","timestamp":1593346273138,"user_tz":-330,"elapsed":104096,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"86662b8d-249b-4578-bb2e-ecba0b185c7c"},"source":["model2=model_lstm_du(embedding_matrix) # create bi-lstm model with attention"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 70)                0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 70, 200)           2492000   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 70, 256)           337920    \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 70, 128)           164864    \n","_________________________________________________________________\n","attention_with_context_1 (At (None, 128)               16640     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 130       \n","=================================================================\n","Total params: 3,019,810\n","Trainable params: 527,810\n","Non-trainable params: 2,492,000\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QDOAXGWVecXf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1593346273144,"user_tz":-330,"elapsed":104068,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"b7317377-e1d4-4cf4-d695-c9b31156dc01"},"source":["model3=model_gru_du(embedding_matrix) # create bi-gru model with attention"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 70)                0         \n","_________________________________________________________________\n","embedding_2 (Embedding)      (None, 70, 200)           2492000   \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 70, 256)           253440    \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 70, 128)           123648    \n","_________________________________________________________________\n","attention_with_context_2 (At (None, 128)               16640     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 130       \n","=================================================================\n","Total params: 2,894,114\n","Trainable params: 402,114\n","Non-trainable params: 2,492,000\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gl2iyMykOKXI","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test=train_test_split(train_X,train_Y, test_size=0.15, random_state=42) #splitting training and validation data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaHerkfPcuqk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1593346289464,"user_tz":-330,"elapsed":120358,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"82f27bcb-bf50-4840-9193-65c250f75116"},"source":["cp2=ModelCheckpoint('model_lstmattn.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True)\n","history2=model2.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=7, batch_size=32,callbacks=[cp2]) #training bi-lstm with attn with checkpoint"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 1921 samples, validate on 340 samples\n","Epoch 1/7\n","1921/1921 [==============================] - 8s 4ms/step - loss: 0.3376 - accuracy: 0.8995 - val_loss: 0.2989 - val_accuracy: 0.9118\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.91176, saving model to model_lstmattn.hdf5\n","Epoch 2/7\n","1921/1921 [==============================] - 1s 614us/step - loss: 0.3299 - accuracy: 0.9011 - val_loss: 0.2990 - val_accuracy: 0.9118\n","\n","Epoch 00002: val_accuracy did not improve from 0.91176\n","Epoch 3/7\n","1921/1921 [==============================] - 1s 617us/step - loss: 0.3187 - accuracy: 0.9011 - val_loss: 0.2913 - val_accuracy: 0.9118\n","\n","Epoch 00003: val_accuracy did not improve from 0.91176\n","Epoch 4/7\n","1921/1921 [==============================] - 1s 669us/step - loss: 0.3052 - accuracy: 0.9011 - val_loss: 0.2628 - val_accuracy: 0.9118\n","\n","Epoch 00004: val_accuracy did not improve from 0.91176\n","Epoch 5/7\n","1921/1921 [==============================] - 1s 620us/step - loss: 0.2692 - accuracy: 0.9011 - val_loss: 0.2670 - val_accuracy: 0.9118\n","\n","Epoch 00005: val_accuracy did not improve from 0.91176\n","Epoch 6/7\n","1921/1921 [==============================] - 1s 612us/step - loss: 0.2433 - accuracy: 0.9016 - val_loss: 0.2578 - val_accuracy: 0.9118\n","\n","Epoch 00006: val_accuracy did not improve from 0.91176\n","Epoch 7/7\n","1921/1921 [==============================] - 1s 702us/step - loss: 0.2137 - accuracy: 0.9162 - val_loss: 0.2689 - val_accuracy: 0.9029\n","\n","Epoch 00007: val_accuracy did not improve from 0.91176\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ZvwNZY3eT2d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1593346300122,"user_tz":-330,"elapsed":130995,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"720fbab4-4edc-4609-a0b1-7221a1bd3f71"},"source":["cp3=ModelCheckpoint('model_gruattn.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True)\n","history3=model3.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=7, batch_size=32,callbacks=[cp3]) #training bi-gru with attn with checkpoint"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 1921 samples, validate on 340 samples\n","Epoch 1/7\n","1921/1921 [==============================] - 2s 973us/step - loss: 0.3432 - accuracy: 0.8896 - val_loss: 0.2997 - val_accuracy: 0.9118\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.91176, saving model to model_gruattn.hdf5\n","Epoch 2/7\n","1921/1921 [==============================] - 1s 536us/step - loss: 0.3191 - accuracy: 0.9011 - val_loss: 0.2835 - val_accuracy: 0.9118\n","\n","Epoch 00002: val_accuracy did not improve from 0.91176\n","Epoch 3/7\n","1921/1921 [==============================] - 1s 575us/step - loss: 0.2976 - accuracy: 0.9011 - val_loss: 0.2792 - val_accuracy: 0.9118\n","\n","Epoch 00003: val_accuracy did not improve from 0.91176\n","Epoch 4/7\n","1921/1921 [==============================] - 1s 634us/step - loss: 0.2748 - accuracy: 0.9011 - val_loss: 0.3016 - val_accuracy: 0.9118\n","\n","Epoch 00004: val_accuracy did not improve from 0.91176\n","Epoch 5/7\n","1921/1921 [==============================] - 1s 540us/step - loss: 0.2669 - accuracy: 0.9011 - val_loss: 0.3260 - val_accuracy: 0.9118\n","\n","Epoch 00005: val_accuracy did not improve from 0.91176\n","Epoch 6/7\n","1921/1921 [==============================] - 1s 579us/step - loss: 0.2239 - accuracy: 0.9011 - val_loss: 0.3313 - val_accuracy: 0.9118\n","\n","Epoch 00006: val_accuracy did not improve from 0.91176\n","Epoch 7/7\n","1921/1921 [==============================] - 1s 633us/step - loss: 0.1824 - accuracy: 0.9183 - val_loss: 0.3385 - val_accuracy: 0.8971\n","\n","Epoch 00007: val_accuracy did not improve from 0.91176\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H9lMk__SOogQ","colab_type":"code","colab":{}},"source":["pred_y2=model2.predict(test_X) #predicting on testing data for bi-lstm with attn\n","pred_y3=model3.predict(test_X) #predicting on testing data for bi-gru with attn\n","pred_y4=(pred_y2+pred_y3)/2    #predicting on the basis of avg of probabilities for above\n","pred_y2=np.argmax(pred_y2,axis=1)\n","pred_y3=np.argmax(pred_y3,axis=1)\n","pred_y4=np.argmax(pred_y4,axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJ9mVYdVPVgf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":544},"executionInfo":{"status":"ok","timestamp":1593346389208,"user_tz":-330,"elapsed":1278,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"d65fd93f-6613-4171-c325-d67bf3b376a7"},"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(\"METRICS FOR TESTING DATA\")\n","print(\"BiLSTM+ATTENTION\")\n","print(classification_report(test['task_3'],pred_y2))\n","print(\"BiGRU+ATTENTION\")\n","print(classification_report(test['task_3'],pred_y3))\n","print(\"HYBRID By Probablity\")\n","print(classification_report(test['task_3'],pred_y4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["METRICS FOR TESTING DATA\n","BiLSTM\n","              precision    recall  f1-score   support\n","\n","           0       0.23      0.23      0.23        43\n","           1       0.87      0.87      0.87       245\n","\n","    accuracy                           0.77       288\n","   macro avg       0.55      0.55      0.55       288\n","weighted avg       0.77      0.77      0.77       288\n","\n","BiGRU\n","              precision    recall  f1-score   support\n","\n","           0       0.11      0.02      0.04        43\n","           1       0.85      0.97      0.90       245\n","\n","    accuracy                           0.83       288\n","   macro avg       0.48      0.50      0.47       288\n","weighted avg       0.74      0.83      0.78       288\n","\n","HYBRID By Probablity\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.07      0.12        43\n","           1       0.86      0.98      0.92       245\n","\n","    accuracy                           0.85       288\n","   macro avg       0.64      0.53      0.52       288\n","weighted avg       0.79      0.85      0.80       288\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iEWca79YUTgM","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}