{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"glove_task1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1jV51WptjVxy_UAu9WnXV8I3twlCbDL48","authorship_tag":"ABX9TyM2GqmlOzUCDy9D6ThrV0Sl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BMOjUkSLf3Ux","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593341785314,"user_tz":-330,"elapsed":2749,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"29e9ae5f-8179-43f3-a823-22b340d41e7b"},"source":["%tensorflow_version 1.x #use tensorflow magic to use version 1.x in colab"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Usom7M_bFh5a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593341791386,"user_tz":-330,"elapsed":8792,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"fbad791f-2dd9-4ee8-f18b-034a6a99f7d5"},"source":["#imports\n","import pandas as pd\n","from keras.preprocessing import text as keras_text, sequence as keras_seq\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n","from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n","from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n","from keras.optimizers import Adam\n","from keras.models import Model\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.utils import np_utils\n","\n","from keras.layers import *\n","from keras.models import *\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.initializers import *\n","from keras.optimizers import *\n","import keras.backend as K\n","from keras.callbacks import *\n","import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"I9KtFhjfFk-s","colab_type":"code","colab":{}},"source":["train=pd.read_csv(\"/content/drive/My Drive/minor/english_dataset/eng1train.csv\") #traning data\n","test=pd.read_csv(\"/content/drive/My Drive/minor/english_dataset/eng1test.csv\")  #testing data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4th6FGtFyHJ","colab_type":"code","colab":{}},"source":["train_X=train['text']  #training text\n","test_X=test['text']   #testing text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGbg5f_AIyQR","colab_type":"code","colab":{}},"source":["embed_size = 200 # how big is each word vector\n","maxlen = 70 # max number of words in a question to use"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RJyyZMh_eE2","colab_type":"code","colab":{}},"source":["\n","## Tokenize the sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(list(train_X)+list(test_X))\n","train_X = tokenizer.texts_to_sequences(train_X)\n","test_X = tokenizer.texts_to_sequences(test_X)\n","word_index = tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwT1bHCiT1z3","colab_type":"code","colab":{}},"source":["max_features = len(word_index) # how many unique words to use (i.e num rows in embedding vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY_HHF4fGGZR","colab_type":"code","colab":{}},"source":["train_X =pad_sequences(train_X, maxlen=maxlen)  #padding training text to length=70\n","test_X = pad_sequences(test_X, maxlen=maxlen)  #padding testing text to length=70\n","train_Y=train['task_1']    #training label\n","test_Y=test['task_1']       #testing label\n","train_Y=np_utils.to_categorical(train_Y)   #one-hot encoded training label\n","test_Y=np_utils.to_categorical(test_Y)   #one-hot encoded testing label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_E8MwYawH9Ih","colab_type":"code","colab":{}},"source":["def load_glove(word_index):\n","    EMBEDDING_FILE = '/content/drive/My Drive/minor/glove/glove.twitter.27B.200d.txt'   #glove file\n","    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n","    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))     #open glove embediing file\n","    \n","    all_embs = np.stack(embeddings_index.values())\n","    emb_mean,emb_std = -0.005838499,0.48782197   #embedding mean, standard deviation for intializing not found vectors\n","    embed_size = all_embs.shape[1]\n","\n","    # word_index = tokenizer.word_index\n","    nb_words = min(max_features, len(word_index))\n","    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n","    for word, i in word_index.items():\n","        if i >= max_features: continue\n","        embedding_vector = embeddings_index.get(word) #get vector for ith word\n","        if embedding_vector is not None: \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            embedding_vector = embeddings_index.get(word.capitalize())  #if vector for lowercase notfound then try for uppercase\n","            if embedding_vector is not None: \n","                embedding_matrix[i] = embedding_vector\n","    return embedding_matrix "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyGkhEJrQyGL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593341875212,"user_tz":-330,"elapsed":92490,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"1e215668-da93-45ca-eb14-1736f36f030f"},"source":["embedding_matrix = load_glove(word_index)  #create embedding matrix using glove"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"F7Z-2alYIxDJ","colab_type":"code","colab":{}},"source":["def model_cnn(embedding_matrix):\n","    filter_sizes = [1,2,3,5] #varous filter sizes\n","    num_filters = 36\n","\n","    inp = Input(shape=(maxlen,))  #input layer\n","    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","    x = Reshape((maxlen, embed_size, 1))(x) #reshape to make 3d data for conv2d layers.\n","\n","    maxpool_pool = [] #list of layers using different filter sizes\n","    for i in range(len(filter_sizes)):\n","        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n","                                     kernel_initializer='he_normal', activation='relu')(x)\n","        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n","\n","    z = Concatenate(axis=1)(maxpool_pool)   #concat results of all filter sizes\n","    z = Flatten()(z)\n","    z = Dropout(0.1)(z)\n","\n","    outp = Dense(2, activation=\"sigmoid\")(z) #output layer\n","\n","    model = Model(inputs=inp, outputs=outp)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n","    print(model.summary())\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSrJna2Ych4C","colab_type":"code","colab":{}},"source":["def model_lstm_du(embedding_matrix):\n","    inp = Input(shape=(maxlen,))  #input layer\n","    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","    '''\n","    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n","        64*70(maxlen)*2(bidirection concat)\n","    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n","    '''\n","    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n","    avg_pool = GlobalAveragePooling1D()(x)\n","    max_pool = GlobalMaxPooling1D()(x)\n","    conc = concatenate([avg_pool, max_pool]) #concat the two poolings\n","    conc = Dense(64, activation=\"relu\")(conc)\n","    conc = Dropout(0.1)(conc)\n","    outp = Dense(2, activation=\"sigmoid\")(conc) #output layer\n","    model = Model(inputs=inp, outputs=outp)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2H5m8yQem6N","colab_type":"code","colab":{}},"source":["def model_gru_du(embedding_matrix):\n","    inp = Input(shape=(maxlen,)) #input layer\n","    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","    '''\n","    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n","        64*70(maxlen)*2(bidirection concat)\n","    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n","    '''\n","    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n","    avg_pool = GlobalAveragePooling1D()(x)\n","    max_pool = GlobalMaxPooling1D()(x)\n","    conc = concatenate([avg_pool, max_pool]) #concat the ouput of two pooling layers\n","    conc = Dense(64, activation=\"relu\")(conc)\n","    conc = Dropout(0.1)(conc)\n","    outp = Dense(2, activation=\"sigmoid\")(conc) #output layer\n","    model = Model(inputs=inp, outputs=outp)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZaAKrsiecMZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":836},"executionInfo":{"status":"ok","timestamp":1593341883370,"user_tz":-330,"elapsed":100574,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"008085bd-61b7-471b-f2cf-40ece617c8f7"},"source":["model1=model_cnn(embedding_matrix) #create cnn model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 70)           0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 70, 200)      4919800     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 70, 200, 1)   0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 70, 1, 36)    7236        reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 69, 1, 36)    14436       reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 68, 1, 36)    21636       reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 66, 1, 36)    36036       reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 4, 1, 36)     0           max_pooling2d_1[0][0]            \n","                                                                 max_pooling2d_2[0][0]            \n","                                                                 max_pooling2d_3[0][0]            \n","                                                                 max_pooling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 144)          0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 144)          0           flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 2)            290         dropout_1[0][0]                  \n","==================================================================================================\n","Total params: 4,999,434\n","Trainable params: 4,999,434\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GqQPL7Fwcmq-","colab_type":"code","colab":{}},"source":["model2=model_lstm_du(embedding_matrix) #create bilstm model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDOAXGWVecXf","colab_type":"code","colab":{}},"source":["model3=model_gru_du(embedding_matrix) #create bigru model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl2iyMykOKXI","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test=train_test_split(train_X,train_Y, test_size=0.15, random_state=42) #splitting training and validation data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQMSpsBNNm0Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1593341906400,"user_tz":-330,"elapsed":123533,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"aec5bf3a-e873-4ee7-f82d-30eb5a372302"},"source":["cp1=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n","history1=model1.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=7, batch_size=32,callbacks=[cp1]) #training cnn with checkpoint"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 4974 samples, validate on 878 samples\n","Epoch 1/7\n","4974/4974 [==============================] - 9s 2ms/step - loss: 0.6558 - acc: 0.6136 - val_loss: 0.6195 - val_acc: 0.6355\n","\n","Epoch 00001: val_acc improved from -inf to 0.63554, saving model to model_cnn.hdf5\n","Epoch 2/7\n","4974/4974 [==============================] - 2s 367us/step - loss: 0.5353 - acc: 0.7095 - val_loss: 0.6151 - val_acc: 0.6583\n","\n","Epoch 00002: val_acc improved from 0.63554 to 0.65831, saving model to model_cnn.hdf5\n","Epoch 3/7\n","4974/4974 [==============================] - 2s 358us/step - loss: 0.2946 - acc: 0.8844 - val_loss: 0.6823 - val_acc: 0.6651\n","\n","Epoch 00003: val_acc improved from 0.65831 to 0.66515, saving model to model_cnn.hdf5\n","Epoch 4/7\n","4974/4974 [==============================] - 2s 380us/step - loss: 0.1325 - acc: 0.9656 - val_loss: 0.7446 - val_acc: 0.6583\n","\n","Epoch 00004: val_acc did not improve from 0.66515\n","Epoch 5/7\n","4974/4974 [==============================] - 2s 364us/step - loss: 0.0687 - acc: 0.9873 - val_loss: 0.7895 - val_acc: 0.6663\n","\n","Epoch 00005: val_acc improved from 0.66515 to 0.66629, saving model to model_cnn.hdf5\n","Epoch 6/7\n","4974/4974 [==============================] - 2s 385us/step - loss: 0.0494 - acc: 0.9910 - val_loss: 0.8477 - val_acc: 0.6788\n","\n","Epoch 00006: val_acc improved from 0.66629 to 0.67882, saving model to model_cnn.hdf5\n","Epoch 7/7\n","4974/4974 [==============================] - 2s 364us/step - loss: 0.0369 - acc: 0.9932 - val_loss: 0.8551 - val_acc: 0.6481\n","\n","Epoch 00007: val_acc did not improve from 0.67882\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LaHerkfPcuqk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1593341929974,"user_tz":-330,"elapsed":147081,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"d5010d81-a3b3-4ab3-d807-d0d1596d9b90"},"source":["cp2=ModelCheckpoint('model_lstm.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True)\n","history2=model2.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=7, batch_size=32,callbacks=[cp2]) #training bilstm with checkpoint"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 4974 samples, validate on 878 samples\n","Epoch 1/7\n","4974/4974 [==============================] - 3s 700us/step - loss: 0.6472 - accuracy: 0.6271 - val_loss: 0.5959 - val_accuracy: 0.6913\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.69134, saving model to model_lstm.hdf5\n","Epoch 2/7\n","4974/4974 [==============================] - 3s 601us/step - loss: 0.5617 - accuracy: 0.7117 - val_loss: 0.6168 - val_accuracy: 0.6970\n","\n","Epoch 00002: val_accuracy improved from 0.69134 to 0.69704, saving model to model_lstm.hdf5\n","Epoch 3/7\n","4974/4974 [==============================] - 3s 610us/step - loss: 0.4040 - accuracy: 0.8221 - val_loss: 0.6609 - val_accuracy: 0.6515\n","\n","Epoch 00003: val_accuracy did not improve from 0.69704\n","Epoch 4/7\n","4974/4974 [==============================] - 3s 605us/step - loss: 0.1973 - accuracy: 0.9210 - val_loss: 0.9187 - val_accuracy: 0.6401\n","\n","Epoch 00004: val_accuracy did not improve from 0.69704\n","Epoch 5/7\n","4974/4974 [==============================] - 3s 593us/step - loss: 0.0796 - accuracy: 0.9741 - val_loss: 1.1984 - val_accuracy: 0.6469\n","\n","Epoch 00005: val_accuracy did not improve from 0.69704\n","Epoch 6/7\n","4974/4974 [==============================] - 3s 605us/step - loss: 0.0416 - accuracy: 0.9901 - val_loss: 1.5700 - val_accuracy: 0.6708\n","\n","Epoch 00006: val_accuracy did not improve from 0.69704\n","Epoch 7/7\n","4974/4974 [==============================] - 3s 603us/step - loss: 0.0259 - accuracy: 0.9930 - val_loss: 1.5961 - val_accuracy: 0.6503\n","\n","Epoch 00007: val_accuracy did not improve from 0.69704\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ZvwNZY3eT2d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1593341952158,"user_tz":-330,"elapsed":169245,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"cc832a69-c4f7-4664-d655-8826ee5b3926"},"source":["cp3=ModelCheckpoint('model_gru.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True)\n","history3=model3.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=7, batch_size=32,callbacks=[cp3]) #training bigru with checkpoint"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 4974 samples, validate on 878 samples\n","Epoch 1/7\n","4974/4974 [==============================] - 3s 654us/step - loss: 0.6530 - accuracy: 0.6130 - val_loss: 0.6184 - val_accuracy: 0.6595\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.65945, saving model to model_gru.hdf5\n","Epoch 2/7\n","4974/4974 [==============================] - 3s 560us/step - loss: 0.5859 - accuracy: 0.6846 - val_loss: 0.5855 - val_accuracy: 0.6925\n","\n","Epoch 00002: val_accuracy improved from 0.65945 to 0.69248, saving model to model_gru.hdf5\n","Epoch 3/7\n","4974/4974 [==============================] - 3s 558us/step - loss: 0.4240 - accuracy: 0.8068 - val_loss: 0.7175 - val_accuracy: 0.6276\n","\n","Epoch 00003: val_accuracy did not improve from 0.69248\n","Epoch 4/7\n","4974/4974 [==============================] - 3s 572us/step - loss: 0.1943 - accuracy: 0.9264 - val_loss: 0.9171 - val_accuracy: 0.6435\n","\n","Epoch 00004: val_accuracy did not improve from 0.69248\n","Epoch 5/7\n","4974/4974 [==============================] - 3s 568us/step - loss: 0.0753 - accuracy: 0.9789 - val_loss: 1.1070 - val_accuracy: 0.6469\n","\n","Epoch 00005: val_accuracy did not improve from 0.69248\n","Epoch 6/7\n","4974/4974 [==============================] - 3s 569us/step - loss: 0.0457 - accuracy: 0.9879 - val_loss: 1.3200 - val_accuracy: 0.6310\n","\n","Epoch 00006: val_accuracy did not improve from 0.69248\n","Epoch 7/7\n","4974/4974 [==============================] - 3s 557us/step - loss: 0.0252 - accuracy: 0.9936 - val_loss: 1.4457 - val_accuracy: 0.6276\n","\n","Epoch 00007: val_accuracy did not improve from 0.69248\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H9lMk__SOogQ","colab_type":"code","colab":{}},"source":["pred_y1=model1.predict(test_X) #predict on testing data for cnn\n","pred_y2=model2.predict(test_X)  #predict on testing data for bilstm\n","pred_y3=model3.predict(test_X)  #predict on testing data for bigru\n","pred_y4=(pred_y1+pred_y2+pred_y3)/3   #predict on testing data on basis of avg of probablities of above\n","pred_y1=np.argmax(pred_y1,axis=1)\n","pred_y2=np.argmax(pred_y2,axis=1)\n","pred_y3=np.argmax(pred_y3,axis=1)\n","pred_y4=np.argmax(pred_y4,axis=1)\n","pred_y5=(pred_y1+pred_y2+pred_y3)/3   #predict what majority among cnn,gru,lstm say\n","pred_y5=np.round(pred_y5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJ9mVYdVPVgf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":884},"executionInfo":{"status":"ok","timestamp":1593341953612,"user_tz":-330,"elapsed":170661,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"b3a94b2e-4c71-4c3e-f899-37bc32e0da17"},"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(\"METRICS FOR TESTING DATA\")\n","print(\"CNN\")\n","print(classification_report(test['task_1'],pred_y1))\n","print(\"BiLSTM\")\n","print(classification_report(test['task_1'],pred_y2))\n","print(\"BiGRU\")\n","print(classification_report(test['task_1'],pred_y3))\n","print(\"HYBRID By Probablity\")\n","print(classification_report(test['task_1'],pred_y4))\n","print(\"HYBRID By Vote\")\n","print(classification_report(test['task_1'],pred_y5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["METRICS FOR TESTING DATA\n","CNN\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.77      0.56       288\n","           1       0.90      0.67      0.77       865\n","\n","    accuracy                           0.69      1153\n","   macro avg       0.67      0.72      0.66      1153\n","weighted avg       0.78      0.69      0.71      1153\n","\n","BiLSTM\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.64      0.52       288\n","           1       0.86      0.74      0.79       865\n","\n","    accuracy                           0.71      1153\n","   macro avg       0.65      0.69      0.66      1153\n","weighted avg       0.75      0.71      0.72      1153\n","\n","BiGRU\n","              precision    recall  f1-score   support\n","\n","           0       0.42      0.80      0.55       288\n","           1       0.90      0.63      0.75       865\n","\n","    accuracy                           0.68      1153\n","   macro avg       0.66      0.72      0.65      1153\n","weighted avg       0.78      0.68      0.70      1153\n","\n","HYBRID By Probablity\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.72      0.54       288\n","           1       0.88      0.69      0.77       865\n","\n","    accuracy                           0.70      1153\n","   macro avg       0.66      0.71      0.66      1153\n","weighted avg       0.77      0.70      0.72      1153\n","\n","HYBRID By Vote\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.73      0.54       288\n","           1       0.88      0.68      0.77       865\n","\n","    accuracy                           0.69      1153\n","   macro avg       0.66      0.71      0.66      1153\n","weighted avg       0.77      0.69      0.71      1153\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iEWca79YUTgM","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}