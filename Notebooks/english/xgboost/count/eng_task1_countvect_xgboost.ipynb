{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"eng_task1_countvect_xgboost.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"18XdZccq1yy5yCtSs8b3s1uzKlgJlw5Fq","authorship_tag":"ABX9TyO+aMKCT1Mb+stlyQeQ1vJC"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CjHRnnn6IHqw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593399152696,"user_tz":-330,"elapsed":4683,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"da6100d3-b6cf-4c36-9983-0f2521c13616"},"source":["#imports\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","import pandas as pd\n","import xgboost, numpy, textblob, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gnyzkUk5I1q8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593399153544,"user_tz":-330,"elapsed":5495,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"6be64941-a3a3-4653-a73a-42504031ea1f"},"source":["#training data\n","df_train=pd.read_csv('/content/drive/My Drive/minor/english_dataset/eng1train.csv', encoding=\"latin-1\")\n","df_train.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>text</th>\n","      <th>task_1</th>\n","      <th>task_2</th>\n","      <th>task_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hasoc_en_1</td>\n","      <td>dhonikeepstheglove  watch sports minister kire...</td>\n","      <td>1</td>\n","      <td>NONE</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hasoc_en_2</td>\n","      <td>@politico no we should remember very clearly t...</td>\n","      <td>0</td>\n","      <td>HATE</td>\n","      <td>TIN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hasoc_en_3</td>\n","      <td>@cricketworldcup guess who would be the winner...</td>\n","      <td>1</td>\n","      <td>NONE</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hasoc_en_4</td>\n","      <td>corbyn is too politically intellectual for bor...</td>\n","      <td>1</td>\n","      <td>NONE</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hasoc_en_5</td>\n","      <td>all the best to teamindia for another swimming...</td>\n","      <td>1</td>\n","      <td>NONE</td>\n","      <td>NONE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      text_id                                               text  ...  task_2 task_3\n","0  hasoc_en_1  dhonikeepstheglove  watch sports minister kire...  ...    NONE   NONE\n","1  hasoc_en_2  @politico no we should remember very clearly t...  ...    HATE    TIN\n","2  hasoc_en_3  @cricketworldcup guess who would be the winner...  ...    NONE   NONE\n","3  hasoc_en_4  corbyn is too politically intellectual for bor...  ...    NONE   NONE\n","4  hasoc_en_5  all the best to teamindia for another swimming...  ...    NONE   NONE\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"yUBqMn6hJAfa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593399153549,"user_tz":-330,"elapsed":5493,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["#training and validation split\n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_train['text'], df_train['task_1'],random_state=140) "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXU_IWYjJKVU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593399153559,"user_tz":-330,"elapsed":5496,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["# label encode the target variable \n","encoder = preprocessing.LabelEncoder() #label encoder\n","train_y = encoder.fit_transform(train_y) #encoding label for training data\n","valid_y = encoder.fit_transform(valid_y) #encoding label for validation data"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfk6KOPLJUSC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593399154482,"user_tz":-330,"elapsed":6409,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["#testing data\n","df_test=pd.read_csv('/content/drive/My Drive/minor/english_dataset/eng1test.csv', encoding=\"latin-1\") \n","test_y = encoder.fit_transform(df_test['task_1']) #encoding the label of testing data"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1S4rHusJYA0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593399154485,"user_tz":-330,"elapsed":6406,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["# create a count vectorizer object \n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(df_train['text'])\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_count =  count_vect.transform(train_x)  #get count vector features for training data\n","xvalid_count =  count_vect.transform(valid_x)  #get count vector features for validation data\n","x_count =  count_vect.transform(df_test['text'])  #get count vector features for testing data"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_vTH_1aJfp3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593399154490,"user_tz":-330,"elapsed":6402,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["def model(classifier, feature_vector_train, label, feature_vector_valid,feature_vector_test):\n","    # fit the training dataset on the classifier\n","    classifier.fit(feature_vector_train, label)\n","    \n","    # predict the labels on validation dataset\n","    predictions_valid = classifier.predict(feature_vector_valid)\n","\n","    # predict the labels on testing dataset\n","    predictions_test = classifier.predict(feature_vector_test)\n","\n","    print(\"classification report for validation\")\n","    print(metrics.classification_report(predictions_valid,valid_y))\n","    print(\"classification report for testing\")\n","    print(metrics.classification_report(predictions_test,test_y))\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjD6P93UJicq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1593399158024,"user_tz":-330,"elapsed":9913,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"15cbb4c3-0e81-4ae0-ac37-685c496e937c"},"source":["# calling the xgboost classifier model for training ,validation and testing\n","model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc(),x_count.tocsc())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["classification report for validation\n","              precision    recall  f1-score   support\n","\n","           0       0.22      0.72      0.34       183\n","           1       0.94      0.64      0.76      1280\n","\n","    accuracy                           0.65      1463\n","   macro avg       0.58      0.68      0.55      1463\n","weighted avg       0.85      0.65      0.71      1463\n","\n","classification report for testing\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.74      0.53       159\n","           1       0.95      0.83      0.89       994\n","\n","    accuracy                           0.82      1153\n","   macro avg       0.68      0.79      0.71      1153\n","weighted avg       0.88      0.82      0.84      1153\n","\n"],"name":"stdout"}]}]}