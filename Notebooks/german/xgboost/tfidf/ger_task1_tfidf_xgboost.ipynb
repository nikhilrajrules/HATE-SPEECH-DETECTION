{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ger_task1_tfidf_xgboost.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1VaRUp3d4vE59mC3oK2jXJUWeV8OoCryU","authorship_tag":"ABX9TyOCNBnIdpwqOSiCNzNThvWD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CjHRnnn6IHqw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593364003463,"user_tz":-330,"elapsed":4263,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"df2e3e00-175a-4dc4-8149-0430fbda94d8"},"source":["#imports\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","import pandas as pd\n","import xgboost, numpy, textblob, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gnyzkUk5I1q8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593364004857,"user_tz":-330,"elapsed":5593,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"6d0c85e0-26c3-454d-bcac-87002a369433"},"source":["#training data\n","df_train=pd.read_csv('/content/drive/My Drive/minor/german_dataset/german_dataset.tsv',sep='\\t', encoding=\"utf-8\")\n","#balancing training data by redundancy\n","d1=df_train[df_train.task_1=='HOF']\n","print(len(d1))\n","d2=df_train[df_train.task_1=='NOT']\n","print(len(d2))\n","d1=pd.concat([d1,d1,d1,d1,d1,d1],ignore_index=True)\n","print(len(d1))\n","df_train=pd.concat([d1,d2],ignore_index=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["407\n","3412\n","2442\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yUBqMn6hJAfa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593364004863,"user_tz":-330,"elapsed":5578,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["#training and validation split\n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_train['text'], df_train['task_1'],random_state=140) "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXU_IWYjJKVU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593364004866,"user_tz":-330,"elapsed":5561,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["# label encode the target variable \n","encoder = preprocessing.LabelEncoder() #label encoder\n","train_y = encoder.fit_transform(train_y) #encoding label for training data\n","valid_y = encoder.fit_transform(valid_y) #encoding label for validation data"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfk6KOPLJUSC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593364004870,"user_tz":-330,"elapsed":5550,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["#testing data\n","df_test=pd.read_csv('/content/drive/My Drive/minor/german_dataset/hasoc_de_test_gold.tsv',sep='\\t', encoding=\"utf-8\") \n","test_y = encoder.fit_transform(df_test['task_1']) #encoding the label of testing data"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1S4rHusJYA0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593364005264,"user_tz":-330,"elapsed":5929,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["# create a tfidf vectorizer object \n","tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=25999)\n","tfidf_vect.fit(df_train['text'])\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_tfidf =  tfidf_vect.transform(train_x)  #get tfidf vector features for training data\n","xvalid_tfidf =  tfidf_vect.transform(valid_x)  #get tfidf vector features for validation data\n","x_tfidf =  tfidf_vect.transform(df_test['text'])  #get tfidf vector features for testing data"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_vTH_1aJfp3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593364005269,"user_tz":-330,"elapsed":5914,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}}},"source":["def model(classifier, feature_vector_train, label, feature_vector_valid,feature_vector_test):\n","    # fit the training dataset on the classifier\n","    classifier.fit(feature_vector_train, label)\n","    \n","    # predict the labels on validation dataset\n","    predictions_valid = classifier.predict(feature_vector_valid)\n","\n","    # predict the labels on testing dataset\n","    predictions_test = classifier.predict(feature_vector_test)\n","\n","    print(\"classification report for validation\")\n","    print(metrics.classification_report(predictions_valid,valid_y))\n","    print(\"classification report for testing\")\n","    print(metrics.classification_report(predictions_test,test_y))\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjD6P93UJicq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1593364010000,"user_tz":-330,"elapsed":10613,"user":{"displayName":"NIKHIL RAJ","photoUrl":"","userId":"13390581239814715976"}},"outputId":"46fe0d64-8167-4fdf-e56e-31ed29e4996e"},"source":["# calling the xgboost classifier model for training ,validation and testing\n","model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc(),x_tfidf.tocsc())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["classification report for validation\n","              precision    recall  f1-score   support\n","\n","           0       0.63      0.77      0.70       477\n","           1       0.88      0.78      0.83       987\n","\n","    accuracy                           0.78      1464\n","   macro avg       0.76      0.78      0.76      1464\n","weighted avg       0.80      0.78      0.78      1464\n","\n","classification report for testing\n","              precision    recall  f1-score   support\n","\n","           0       0.39      0.24      0.30       220\n","           1       0.77      0.87      0.81       630\n","\n","    accuracy                           0.71       850\n","   macro avg       0.58      0.55      0.56       850\n","weighted avg       0.67      0.71      0.68       850\n","\n"],"name":"stdout"}]}]}